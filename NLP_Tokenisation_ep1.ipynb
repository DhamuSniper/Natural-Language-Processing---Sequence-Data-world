{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Tokenisation_ep1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DhamuSniper/Natural-Language-Processing---Sequence-Data-world/blob/master/NLP_Tokenisation_ep1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AU8WDEo296TA",
        "colab_type": "text"
      },
      "source": [
        "Hello! Natural Language Processing is giving voice to the computers. It gives machines the ability to speak, understand and process our language. But we are far away from what we see in Tony Stark's JARVIS. We have Alexa from Amazon, Siri from Apple, Okay Google from Google and lot more products.\n",
        "\n",
        "Computer Scientists are working hard to make clever intelligent bots all over the world. \n",
        "\n",
        "As an Engineers, Let's join them. In this series of github codes, I am going to code in Python with Tensorflow framework for simpler NLP tasks as four episodes, purely inspired from Andrew Ng but in my way with some help of Tensorflow Docs! \n",
        "\n",
        "At the end of four episodes, you can tell yourself as beginner in NLP(But I shall assure you that you cannot create Neural nets but somehow you will understand what happening here(smile please) )\n",
        "\n",
        "Let's Begin!\n",
        "\n",
        "First task: Tokenising the text!\n",
        "\n",
        "Hey! Ask yourself a question. \n",
        "\n",
        "\n",
        "1.   Why do I want to tokenise my text? \n",
        "\n",
        "2.   First of all, What is Tokenization? \n",
        "\n",
        "3.   Why it is in NLP?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Wl3eu62CLXb",
        "colab_type": "text"
      },
      "source": [
        "Hey, you know Neural Nets are designed in the way that only accepts numbers for it's operations(weight*feature multiplications, biasing,for loss calculations, for backprogation calculations). If it is an image data, then pixels values( actually numbers ) can be used for feature inputs. But this is text, NLP is all about text. \n",
        "                                                Hence TEXT data has to be preprocessed. Text data must converted as vectors( actually numbers (refer: scalars and vectors in linear algebra)) of matrix. \n",
        "                                                \n",
        "Preprocessing of Text involves:\n",
        "\n",
        "\n",
        "1.   Tokenization\n",
        "2.   Removing special characters, stop words\n",
        "3.   Lowercase conversion\n",
        "4.   Stemming & Lemmetization\n",
        "5.   POS tagging for NER and lot more\n",
        "\n",
        "\n",
        "Still I didn't explain What and why for Tokenization is! \n",
        "\n",
        "Let's add few more ingredients to taste this. I told you that I need to have numbers for text. We have already numbers for characters in computer science ie, ASCII. \n",
        "\n",
        "But we must not use ASCII for text to vector(number) conversion. Why? Let's take a word \"dog\" d-->100 o-->111 g-->103. And let's take the same word like this \"DOG\" D-->68 O-->79 G-->71. For the word \"dog\"==>[100 111 103] and for the word \"DOG\"==>[68 79 71]. Does they resemble same? Although the meanings for the two words are same, they differ on ASCII based vector values. Hence this traditional way will not help neural networks to learn words.\n",
        "\n",
        "So the need for Tokenisation comes here. Let's take the sentence \"I love you Veronica\" Here we have four words such as \"I\", \"love\", \"you\", \"Veronica\". Machine cannot understand this. So we are giving the values randomly such as I to 01, love to 02, you to 03, Veronica to 04. \n",
        "                      To give this number, we need to tokenize(\"It is the process of choppin individual words from document or text\"). This is why we need tokenization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fzohq6Ma9hVe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Let's Import the needs\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcx95MRsMRqD",
        "colab_type": "code",
        "outputId": "867765f5-0134-4d0f-931e-5b40299dc91f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sentences=['I love IRONMAN',\n",
        "          'I love you 3000',\n",
        "          ]\n",
        "\n",
        "tokenizer=Tokenizer(num_words=100)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index=tokenizer.word_index\n",
        "\n",
        "\n",
        "print(word_index)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'i': 1, 'love': 2, 'ironman': 3, 'you': 4, '3000': 5}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQDtaKGWNAzW",
        "colab_type": "text"
      },
      "source": [
        "This the Output:  \n",
        "                  {'i': 1, 'love': 2, 'ironman': 3, 'you': 4, '3000': 5}\n",
        "                  \n",
        "Have you noticed? that I have given I and IRONMAN as in capital letters but the output has different ones. Tensorflow takes care of one of the preprocessing called lower-casing as we discussed earlier.\n",
        "\n",
        "\n",
        "In Python's syntax, It is called as Dictionary(output). Now you may call this as Corpus(preprocessed text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJFNa9FgPRn7",
        "colab_type": "text"
      },
      "source": [
        "We tokenized our text. Let's see how to convert that into Sequence( which is actually matrix values that Neural nets can understand)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DF175MTTP3T_",
        "colab_type": "code",
        "outputId": "d054d461-8534-490a-e92c-e62fe40f1c96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "print(sequences)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 2, 3], [1, 2, 4, 5]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDWSmIP_P9d-",
        "colab_type": "text"
      },
      "source": [
        "Now, I love IRONMAN is [1,2,3] and I love you 3000 is [1,2,4,5]. Yes, Now we can feed this into neural network. Wait!!! still one more issue.... Let's see"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07jqntzHP6LK",
        "colab_type": "code",
        "outputId": "e2df167d-d8fa-47b4-965d-927e874a24da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#Textual Data is completely different from one another...Let's see how this Tensorflow API is going to differentiate this!\n",
        "\n",
        "New_sentences=['I am Dhamu',\n",
        "              ' Do you know me',\n",
        "              'love my friends',\n",
        "              \"Heloo new sentence\"]\n",
        "\n",
        "New_tokenizer=Tokenizer(num_words=100)\n",
        "New_tokenizer.fit_on_texts(New_sentences)\n",
        "New_word_index=New_tokenizer.word_index\n",
        "\n",
        "New_Sequence=New_tokenizer.texts_to_sequences(New_sentences)\n",
        "print(New_word_index)\n",
        "print(New_Sequence)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'i': 1, 'am': 2, 'dhamu': 3, 'do': 4, 'you': 5, 'know': 6, 'me': 7, 'love': 8, 'my': 9, 'friends': 10, 'heloo': 11, 'new': 12, 'sentence': 13}\n",
            "[[1, 2, 3], [4, 5, 6, 7], [8, 9, 10], [11, 12, 13]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YO8d-bx_SOez",
        "colab_type": "text"
      },
      "source": [
        "Output:  [[1, 2, 3], [4, 5, 6, 7], [8, 9, 10], [11, 12, 13]]... Yes,it is successfully differentiated by Tensorflow APIs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jskX9oY4g-Tl",
        "colab_type": "text"
      },
      "source": [
        "**Playing tokenization with REAL WORLD DATA**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyFeiuSChbQ2",
        "colab_type": "text"
      },
      "source": [
        "We shall use the Dataset called SARCASM prediction from News Headlines owned by,\n",
        "\n",
        "citation: Sarcasm Detection using Hybrid Neural Network [download](https://https://www.kaggle.com/rmisra/news-headlines-dataset-for-sarcasm-detection#Sarcasm_Headlines_Dataset.json)\n",
        "Rishabh Misra, Prahal Arora\n",
        "Arxiv, August 2019\n",
        "[Google Scholar link of Rishabh Misra, Machine learning Engineer, Twitter](https://https://scholar.google.com/citations?view_op=list_works&hl=en&user=EN3OcMsAAAAJ#)\n",
        "\n",
        "Thanks to him. Let's continue this. Our job here to import the data and tokenize it.\n",
        "\n",
        "Let's do this!\n",
        "\n",
        "\n",
        "(Will be continued tomorrow... Have a great Day! Checkout for git commits!)"
      ]
    }
  ]
}